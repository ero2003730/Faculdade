{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ero2003730/Faculdade/blob/main/Enzo_Reis_de_Oliveira_Renan_Leite_Atividade_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnj9u3vrGEYX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgnPyxYpSlBv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqcN4rgqShWv"
      },
      "outputs": [],
      "source": [
        "#forma de abrir do renan\n",
        "zf1 = zipfile.ZipFile('/content/drive/MyDrive/House_Sale/House_Sale.zip')  #aloca o arquivo zip\n",
        "\n",
        "data = pd.read_csv(zf1.open('kc_house_data.csv')) # abre o arquivo CSV 'train.csv' presente dentro do ZIP\n",
        "\n",
        "#forma de abrir do enzo\n",
        "#zf2 = zipfile.ZipFile('/content/drive/MyDrive/Quinto Semestre/IA/Atividade_2/House_Sale/House_Sale.zip')  #aloca o arquivo zip\n",
        "\n",
        "#data = pd.read_csv(zf2.open('kc_house_data.csv')) # abre o arquivo CSV 'train.csv' presente dentro do ZIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHoXtIEhQX1Z"
      },
      "outputs": [],
      "source": [
        "data_total = data\n",
        "data = data.drop(['id', 'date'], axis = 1) # id e data são dados irrelevantes para o preço. Logo, retirar essas 2 colunas\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_oV0I2tsXsA"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum() # iremos ver se tem algum campo com valores NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qez21k41sbFg"
      },
      "outputs": [],
      "source": [
        "# Como existem campos com NaN, iremos descobrir quais são as posições, e iremos \n",
        "# remover essas linhas\n",
        "\n",
        "np.where(data['sqft_above'].isnull().values==True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayXKfK9TslLf"
      },
      "outputs": [],
      "source": [
        "# Dados faltantes nas posições 4670 e 7357, então iremos remover essas linhas\n",
        "data = data.drop([10, 17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaIlDdY1RlJa"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac = 1) # aqui iremos embaralhar os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLOXQlqBQviq"
      },
      "outputs": [],
      "source": [
        "posicao_latitude = data['lat'].values # iremos salvar os dados da latitude \n",
        "print(posicao_latitude)\n",
        "posicao_longitude = data['long'].values # iremos salvar os dados da longitude\n",
        "print(posicao_longitude)\n",
        "\n",
        "# esses dados serão importantes para depois vincularmos às respectivacs posições no mapa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPrtyyFjTu-g"
      },
      "outputs": [],
      "source": [
        "# como já salvamos os valores da latitude e longiute, iremos apagar essas 2 colunas, pois não são relevantes para o nosso preço\n",
        "data = data.drop(['lat', 'long'], axis = 1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QtO5m-Pr0uG"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzSWS8s7TPbc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "corr = data.corr()\n",
        "corr\n",
        "\n",
        "f, ax = plt.subplots(figsize = (15,12))\n",
        "sns.heatmap(corr, cmap = sns.color_palette(\"Greens\", as_cmap=True), linewidths = 5, annot = True) \n",
        "\n",
        "# aqui estamos analisando a correlação. O objetivo da análise dessa correlação é analisar\n",
        "# a correlação entre as amostras. Para nosso algoritmo, não é importante correlações próxs\n",
        "# a 1, ou ou a -1. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6bMnue-UPZZ"
      },
      "outputs": [],
      "source": [
        "# ao analisar as correlações percebe-se que o zipcode não é relevante\n",
        "# para nosso algoritmo, pois toda sua linha está beirando o 0, então\n",
        "# retira-lo\n",
        "\n",
        "data = data.drop(['zipcode'], axis = 1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpJpNL6GmvtF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split # separa os dados em treinamento e teste de forma aleatória\n",
        "\n",
        "y = data['price']\n",
        "X = data.iloc[:, 1:16]\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "\n",
        "# Iremos ter 80% para treinamento e 20% para teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L1LLrInomaF"
      },
      "outputs": [],
      "source": [
        "# Aqui estamos utilizando o método linear para fazer a predição\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn import metrics\n",
        "\n",
        "regr = linear_model.LinearRegression() # Aqui estamos criando o modelo de regressão linear\n",
        "regr.fit(X_train, y_train) # Aqui estamos treinando o modelo\n",
        "regr_predicao = regr.predict(X_test) # Aqui estmaos fazendo uma predição\n",
        "\n",
        "# Calcula as métricas MAE, MSE e RMSE entre as predições do modelo e os valores verdadeiros\n",
        "MAE_regr = metrics.mean_absolute_error(y_test, regr_predicao) \n",
        "MSE_regr = metrics.mean_squared_error(y_test, regr_predicao) \n",
        "RMSE_regr = np.sqrt(metrics.mean_squared_error(y_test, regr_predicao)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui estamos utilizando o método de árvore para fazer a predição\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "clf = tree.DecisionTreeRegressor(random_state=0, max_depth=5) # Instancia um objeto da classe DecisionTreeRegressor, com um valor fixo para random_state e um limite de profundidade de 5 camadas\n",
        "\n",
        "clf = clf.fit(X_train, y_train) # Treina o modelo DecisionTreeRegressor nos dados de treinamento\n",
        "\n",
        "dt_predicao_arvore = clf.predict(X_test) # Realiza a predição do modelo DecisionTreeRegressor nos dados de teste\n",
        "\n",
        "# Calcula as métricas MAE, MSE e RMSE entre as predições do modelo e os valores verdadeiros\n",
        "MAE_tree = metrics.mean_absolute_error(y_test, dt_predicao_arvore)\n",
        "MSE_tree = metrics.mean_squared_error(y_test, dt_predicao_arvore)\n",
        "RMSE_tree = np.sqrt(metrics.mean_squared_error(y_test, dt_predicao_arvore))\n",
        "\n",
        "# Plota a árvore de decisão construída\n",
        "tree.plot_tree(clf)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QKFOWr36voCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzFmWvNJVFSB"
      },
      "outputs": [],
      "source": [
        "# Importar as bibliotecas necessárias\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Separar a variável alvo e as features\n",
        "y = data['price'] # vamos salvar o preço \n",
        "X = data.iloc[:, 1:16] # aqui iremos salvar todos os dados das colunas 1 - 16\n",
        "\n",
        "# Selecionar as melhores variáveis usando o teste F\n",
        "selector = SelectKBest(score_func = f_regression, k=8)\n",
        "\n",
        "# Aplicar a seleção de atributos aos dados de entrada X e às saídas y\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.values)\n",
        "X_selected = selector.fit_transform(X_scaled, y)\n",
        "\n",
        "# Dividir o conjunto de dados em 5 partes para validação cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "kf.get_n_splits(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Definindo as camadas da rede neural\n",
        "        self.fc1 = nn.Linear(15, 50) # camada totalmente conectada com 15 entradas e 50 saídas\n",
        "        self.bn1 = nn.BatchNorm1d(50) # normalização em lote para regularizar a rede\n",
        "        self.fc2 = nn.Linear(50, 25) # camada totalmente conectada com 50 entradas e 25 saídas\n",
        "        self.bn2 = nn.BatchNorm1d(25) # normalização em lote para regularizar a rede\n",
        "        self.fc3 = nn.Linear(25, 15) # camada totalmente conectada com 25 entradas e 15 saídas\n",
        "        self.bn3 = nn.BatchNorm1d(15) # normalização em lote para regularizar a rede\n",
        "        self.fc4 = nn.Linear(15, 10) # camada totalmente conectada com 15 entradas e 10 saídas\n",
        "        self.bn4 = nn.BatchNorm1d(10) # normalização em lote para regularizar a rede\n",
        "        self.fc5 = nn.Linear(10, 1) # camada totalmente conectada com 10 entradas e 1 saída\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Propagação para frente na rede neural\n",
        "        x = nn.functional.relu(self.bn1(self.fc1(x))) # camada 1\n",
        "        x = nn.functional.relu(self.bn2(self.fc2(x))) # camada 2\n",
        "        x = nn.functional.relu(self.bn3(self.fc3(x))) # camada 3\n",
        "        x = nn.functional.relu(self.bn4(self.fc4(x))) # camada 4\n",
        "        x = self.fc5(x) # camada de saída\n",
        "        return x"
      ],
      "metadata": {
        "id": "DGZUfxYgacZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQnmecdoc_JU"
      },
      "outputs": [],
      "source": [
        "# Defina a função para treinar a RNA\n",
        "def train(model, X_train, y_train, X_val, y_val, num_epoch=1000, optimizer_type=\"Newton\", weight_decay=0.001, patience=30):\n",
        "    criterion = nn.MSELoss() # Define a função de perda como o erro quadrático médio\n",
        "\n",
        "    # Define o otimizador com base no tipo de otimizador passado como parâmetro\n",
        "    if optimizer_type == \"Newton\":\n",
        "        optimizer = optim.Rprop(model.parameters(), lr=0.1)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    \n",
        "    best_val_loss = float('inf') # Define a melhor perda na validação inicialmente como infinito\n",
        "    best_model = None # Define o melhor modelo como vazio\n",
        "    patience_counter = 0 # Inicializa o contador de paciência como zero\n",
        "    train_losses = [] # Inicializa uma lista vazia para armazenar as perdas de treinamento\n",
        "    val_losses = [] # Inicializa uma lista vazia para armazenar as perdas de validação\n",
        "\n",
        "    # Loop de treinamento\n",
        "    for epoch in range(num_epoch):\n",
        "        optimizer.zero_grad() # Zera os gradientes do otimizador\n",
        "        outputs = model(X_train) # Realiza a predição do modelo nos dados de treinamento\n",
        "        loss = criterion(outputs, y_train) # Calcula a perda com base na função de perda e nas predições e rótulos de treinamento\n",
        "        l2_reg = torch.tensor(0.) # Inicializa a regularização L2 como zero\n",
        "        for param in model.parameters(): # Loop pelos parâmetros do modelo\n",
        "            l2_reg += torch.norm(param) # Adiciona a norma L2 dos parâmetros à regularização L2\n",
        "        loss += weight_decay * l2_reg # Adiciona a regularização L2 à perda\n",
        "        loss.backward() # Realiza a propagação de volta do gradiente para calcular os gradientes dos parâmetros\n",
        "        optimizer.step() # Realiza uma etapa de otimização para atualizar os parâmetros com base nos gradientes\n",
        "\n",
        "        train_losses.append(loss.item()) # Adiciona a perda de treinamento à lista de perdas de treinamento\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val) # Realiza a predição do modelo nos dados de validação\n",
        "            val_loss = criterion(val_outputs, y_val) # Calcula a perda com base na função de perda e nas predições e rótulos de validação\n",
        "            val_losses.append(val_loss.item()) # Adiciona a perda de validação à lista de perdas de validação\n",
        "\n",
        "        if val_loss < best_val_loss: # Verifica se a perda de validação atual é menor que a melhor perda de validação até agora\n",
        "            best_val_loss = val_loss # Atualiza a melhor perda de validação\n",
        "            best_model = model.state_dict() # Salva os parâmetros do modelo correspondentes à melhor perda de validação\n",
        "            patience_counter = 0 # Reseta o contador de paciência\n",
        "        else:\n",
        "            patience_counter += 1 # Incrementa o contador de paciência\n",
        "            if patience_counter >= patience: # Verifica se o contador de paciência excedeu o limite de paciência\n",
        "                print(\"Early stopping after epoch\", epoch) # Imprime a mensagem de interrupção antecipada\n",
        "                model.load_state_dict(best_model) # Carrega o modelo\n",
        "                break\n",
        "    \n",
        "    return model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdZvoNeLXBFc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "num_epochs = 1000\n",
        "models = []\n",
        "results = []\n",
        "\n",
        "\n",
        "# Itera sobre os folds gerados pelo K-Fold\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X_scaled)):\n",
        "    \n",
        "    # Define os conjuntos de treinamento e teste para esta iteração\n",
        "    X_train = torch.from_numpy(X_scaled[train_index]).float()\n",
        "    y_train = torch.from_numpy(y.values[train_index]).float().unsqueeze(1)\n",
        "    X_test = torch.from_numpy(X_scaled[test_index]).float()\n",
        "    y_test = torch.from_numpy(y.values[test_index]).float().unsqueeze(1)\n",
        "\n",
        "    # Cria uma nova instância do modelo de rede neural\n",
        "    model = Net()\n",
        "\n",
        "    # Treina o modelo com os dados de treinamento\n",
        "    model, train_loss, val_loss = train(model, X_train, y_train, X_test, y_test, num_epoch=num_epochs, weight_decay=0.001)\n",
        "\n",
        "    # Realiza a predição dos dados de teste\n",
        "    y_pred = model(X_test)\n",
        "\n",
        "    # Calcula o erro absoluto percentual médio (MAPE)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred.detach().numpy())\n",
        "\n",
        "    # Armazena os resultados desta iteração\n",
        "    result = {\n",
        "        'model': model,\n",
        "        'mape': mape,\n",
        "        'train_losses': train_loss,\n",
        "        'val_losses': val_loss\n",
        "    }\n",
        "    results.append(result)\n",
        "\n",
        "# Ordena os modelos de acordo com seus valores MAPE (de tras para frente)\n",
        "models_sorted = sorted(results, key=lambda x: x['mape'], reverse = False)\n",
        "\n",
        "# Seleciona os 5 melhores modelos\n",
        "top_models = [r['model'] for r in models_sorted[:5]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLzCmEOPKiUJ"
      },
      "outputs": [],
      "source": [
        "# Aqui iremos salvar as 5 melhores redes no modelo em que foi pedido\n",
        "\n",
        "for i, model in enumerate(top_models):\n",
        "    filename = f'Enzo_Reis_e_Renan_Leite_RNA_FOLD_{i+1}.pth'\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    print(f'Modelo {i+1}: {filename}, MAPE: {models_sorted[i][\"mape\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrUX1IFosJFp"
      },
      "outputs": [],
      "source": [
        "# Aqui estamos plotando 5 gráficos, que mostra como a Loss se comporta através das épocas\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, model in enumerate(top_models):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(results[i]['train_losses'], label='Training Loss')\n",
        "    plt.plot(results[i]['val_losses'], label='Validation Loss')\n",
        "    plt.title('Model ' + str(i+1) + ' Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui está sendo mostrado um histograma da diff pedida também \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "diff_all = []\n",
        "mape_all = []\n",
        "for i in range(len(top_models)):\n",
        "    model = top_models[i]\n",
        "    y_pred = model(X_test)\n",
        "    diff = np.absolute(y_test - y_pred.detach().numpy())\n",
        "    diff_all.extend(diff)\n",
        "    mape_all.append(models_sorted[i]['mape'])\n",
        "\n",
        "plt.hist(diff_all, bins=20)\n",
        "plt.title(f\"Folds: {np.mean(mape_all):.4f}\")\n",
        "plt.xlabel('Diff Preço')\n",
        "plt.ylabel('Frequencia')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tfvfrFIzm40n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Kk2jivnpGe"
      },
      "outputs": [],
      "source": [
        "# Aqui está sendo plotado o comportamento do preço predito pelo algoritmo e o preço real, pelos 5 folds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for model in top_models:\n",
        "    y_pred = model(X_test)\n",
        "    fig, ax = plt.subplots(figsize=(16,8))\n",
        "    ax.plot(y_pred.detach().numpy()[:50], label = 'Predicted')\n",
        "    ax.plot(y_test.detach().numpy()[:50], label = 'Real')\n",
        "    ax.set_xlabel('Amostras')\n",
        "    ax.set_ylabel('Preco')\n",
        "    ax.set_title('Predicao Modelo vs Valores Reais')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afIKTv-yoXz0"
      },
      "outputs": [],
      "source": [
        "# Aqui iremos plotar a comparação entre os 3 modelos gerados \n",
        "# Regressão Linear, Árvore e Redes Neurais\n",
        "\n",
        "y_pred = model(X_test)\n",
        "MAE_RNA = metrics.mean_absolute_error(y_test, y_pred.detach().numpy())\n",
        "MSE_RNA = metrics.mean_squared_error(y_test, y_pred.detach().numpy())\n",
        "RMSE_RNA = np.sqrt(metrics.mean_squared_error(y_test, y_pred.detach().numpy()))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Coloque os valores de modelo e as métricas em um dicionário\n",
        "model_dict = {'Modelo': ['Regressão Linear', 'Árvore de Decisão', 'Redes Neurais'],\n",
        "              'MAE': [MAE_regr, MAE_tree, MAE_RNA],\n",
        "              'MSE': [MSE_regr, MSE_tree, MSE_RNA],\n",
        "              'RMSE': [RMSE_regr, RMSE_tree, RMSE_RNA]}\n",
        "\n",
        "# Crie um dataframe a partir do dicionário\n",
        "df = pd.DataFrame(model_dict)\n",
        "\n",
        "# Imprima o novo dataframe\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui iremos colocar a coluna \"NEW PRICE\" em nosso data frame\n",
        "\n",
        "# Selecionar o melhor modelo da lista de modelos ordenados\n",
        "best_model = models_sorted[0]['model']\n",
        "\n",
        "# Utilizar o modelo para prever os valores de y para todo o conjunto de dados X\n",
        "X_tensor = torch.from_numpy(X_scaled).float()\n",
        "y_pred = best_model(X_tensor)\n",
        "y_pred = y_pred.detach().numpy()\n",
        "\n",
        "print(len(data_total))\n",
        "print(len(y_pred))\n",
        "\n",
        "# data = data.iloc[:-2]\n",
        "data_total['NEW PRICE'] = y_pred"
      ],
      "metadata": {
        "id": "9P1fimwXC04O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdkqr2gt-JXD"
      },
      "outputs": [],
      "source": [
        "data_total.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hYfAmgRyZ6m"
      },
      "outputs": [],
      "source": [
        "data_total[[\"price\", \"NEW PRICE\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para visualização do mapa basta baixar o arquivo e rodar ele depois no navegador\n",
        "\n",
        "<a href=\"https://drive.google.com/file/d/1XfChxUh38MVXG5vBblxx2B1lpvoOiko_/view?usp=share_link\">CLIQUE AQUI PARA BAIXAR</a>\n"
      ],
      "metadata": {
        "id": "GzmlmJ9vZPo1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}